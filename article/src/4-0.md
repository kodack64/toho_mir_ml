# さいごに

---

### ソースコードとか
ソースコードは
[http://github.com/kodack64/toho_mir_ml.git](http://github.com/kodack64/toho_mir_ml.git)
にあります。言い訳になりますが書き捨てであまり再利用することを意図していないのと、依存する外部ライブラリが死ぬほど多いので、動かすのは大変だと思います。

やってることは基本的にライブラリで抽出してscikit-learnに投げる、なので、ライブラリのインストール方法を書いたので自分でやった方が早いと思います。Windows+Python3上で仮想環境をぶん回して作りましたが不要な地獄を大量に生みました。悪いことは言わないのでLinux+Python2でやりましょう。


### 所感

VSTとかASIOとかを趣味で弄って自動作曲のプロジェクトに参加したり趣味でDTM他エフェクト作ったりコード進行の分析とかをしていたのは7年前とかなので、知識も薄れ言語もC++からpythonになり思ったより苦労するかと思いきや、pythonのライブラリが超親切で想像よりかなりサクサク書けました。.wavファイルのファイルフォーマットをいちいちググらなくてよいってのは非常に良い時代になりましたね。

いわゆる機械学習に興味を持ち始めたのは逐次学習を扱うバイトのようなものを少しやったり、[word2vecのこの記事をみた](http://antibayesian.hateblo.jp/entry/2014/03/10/001532)を読んで面白そうと思ってサンプル少しいじる程度だったので、手法の妥当性はぶっちゃけあんま自信ないです。何かおかしいことをしていたら（コミケの記事はもう手遅れですが）後学のために是非教えてください。

技術的にはstackingで音楽の識別をやってみたかったという動機があったので、とりあえず一応目標が達成できて満足です。stackingはkaggle界隈のブログを見ていて初めて用語を知りましたが、自分の周辺だとあんまりkaggle界隈以外で見ないワードなんですが、データ分析をするうえで結構重要なアプローチに見えます。どうなんでしょう。

音声認識といえばRNNがナウいという見た目があるので、楽曲の認識でも使えるんちゃうかと思ったんですがKerasのインストールでこけました。そもそもNNをどうやってスタックしていくのかとか全然わからんし試す時間もなかなかないので出せるような結果はないです。でも[magenta](https://magenta.tensorflow.org/welcome-to-magenta)とかは一時期自動作曲とかの話をやらせてもらってたこともあって結構興味あるので今後余裕できたらキャッチアップしたいですね。


### ライセンス
寄稿した記事以外の、俺が作った部分のプログラムや結果はパブリックドメインです。好きにしてください。

GTZANとMagnaTagATuneのデータセットは以下からの引用です。

GTZAN
> " Musical genre classification of audio signals " by G. Tzanetakis and P. Cook in IEEE Transactions on Audio and Speech Processing 2002.

MagnaTagATune
> Edith Law, Kris West, Michael Mandel, Mert Bay and J. Stephen Downie (2009). Evaluation of algorithms using games: the case of music annotation. In  Proceedings of the 10th International Conference on Music Information Retrieval (ISMIR)

東方プロジェクトおよびその楽曲は上海アリス幻樂団様の作品です。アニソンも同様に作曲者の方の作品です。

使用した曲名の一覧、前処理済みデータ、学習済みモデルは一般的にどのあたりまで配布してよいのか判断がつきかねるので、配布しないことにしています。すいません。
東方でない曲のデータ群は特別にこのデータが重要である、というようなことはないと思いますので、是非自分で作ってみてください。
